{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TestCustom.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOt0D86ihyRHz1ngjXDjdNW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmmovania/CUDA_Spring2023_GoogleColab/blob/main/Week1/TestCustom.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This jupyter notebook shows you how to compile CUDA code manually. We first do the required setup of symbolic links to enable compilation of CUDA code inside jupyter notebook environment. "
      ],
      "metadata": {
        "id": "F3zI_PbsM4iX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTuaHG0vryRN",
        "outputId": "a003ad76-afa0-4d3a-8370-515af0664e92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local\n",
            "  File: cuda -> /usr/local/cuda-11.2\n",
            "  Size: 20        \tBlocks: 0          IO Block: 4096   symbolic link\n",
            "Device: 34h/52d\tInode: 2490373     Links: 1\n",
            "Access: (0777/lrwxrwxrwx)  Uid: (    0/    root)   Gid: (    0/    root)\n",
            "Access: 2023-01-06 04:06:00.099784178 +0000\n",
            "Modify: 2023-01-06 04:05:59.982775528 +0000\n",
            "Change: 2023-01-06 04:05:59.982775528 +0000\n",
            " Birth: -\n"
          ]
        }
      ],
      "source": [
        "%cd /usr/local\n",
        "!rm -rf cuda\n",
        "!ln -s /usr/local/cuda-11.2 /usr/local/cuda\n",
        "!stat cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we try to run the nvcc compiler for compiling C code inside Google colab environment. If everything goes well, we should get the version of the nvcc compiler written on the output."
      ],
      "metadata": {
        "id": "VCiN1wGYNlij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaNUb1B8tGK6",
        "outputId": "29c2cbd1-1ed3-4657-9211-231d2c8d2f51"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2021 NVIDIA Corporation\n",
            "Built on Sun_Feb_14_21:12:58_PST_2021\n",
            "Cuda compilation tools, release 11.2, V11.2.152\n",
            "Build cuda_11.2.r11.2/compiler.29618528_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we try to compile our own code test.cu. Before running this cell, please ensure that there is a file test.cu in your content folder on the left side. You can either create the test.cu file manually and upload it or you can generate it. We will do the later process of generating the code from jupyter notebook. Run the following cell and it should output test.cu insides your content folder. "
      ],
      "metadata": {
        "id": "eqHVk6coNyDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/test.cu\n",
        "//test.cu generated using jupyter colab environment\n",
        "#include <stdio.h>\n",
        "\n",
        "//define our kernel function\n",
        "__global__ void HelloKernel() {\n",
        "    printf(\"\\tHello from GPU (device)\\n\");\n",
        "}\n",
        "\n",
        "//define our main function\n",
        "int main() {\n",
        "  printf(\"Hello from CPU (host) before kernel execution\\n\");\n",
        "  HelloKernel<<<1,32>>>();\n",
        "  cudaDeviceSynchronize();\n",
        "  printf(\"Hello from CPU (host) after kernel execution\\n\");\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "ET5WUljjO-OP",
        "outputId": "79695c99-7938-4dc3-968a-6e4c9ae0c233",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/test.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then compile the generated code file /content/test.cu by using the command nvcc input_filename -o output_filename."
      ],
      "metadata": {
        "id": "HH_VPSa9PuEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc /content/test.cu -o test"
      ],
      "metadata": {
        "id": "sAHeA1vLtJ5q"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the code is syntactically correct, the output should not have anything in it. We then try running the code using ./test file since this was the name of the output file during compilation. Note that our current directory is the content folder so we donot have to prepend the /content/ folder to the filename. If all goes well, we should get the two statements from CPU and within them the GPU outputs will be sandwitched."
      ],
      "metadata": {
        "id": "2q0qb8P1P2W3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!./test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heFe3XJIu-0r",
        "outputId": "0ce567ba-3d46-40fd-ba4e-7b361e65d76f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello from CPU (host) before kernel execution\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "Hello from CPU (host) after kernel execution\n"
          ]
        }
      ]
    }
  ]
}